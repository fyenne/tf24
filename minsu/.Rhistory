length(df_train)
dim(df_train)
train <- read.csv("./train_data.csv")
target <- read.csv("./target.csv")
head(train)
# train['X18']
train['X88'] = lapply(train['X18'], FUN = substr, start = 1, stop = 5)
# train = train[,! names(train) %in% 'X18']
train = train[,! names(train) %in% c('X18', 'X21', 'X22', 'X23')]
df_train = df_train[ , !names(df_train) %in% c("X13","X88")]
for (i in length(df_train)){
which(class(df_train[,i]) == "cha")
}
for (i in dim(df_train)){
which(class(df_train[,i]) == "cha")
}
for (i in dim(df_train)[2]){
which(class(df_train[,i]) == "cha")
}
which(class(df_train[,i]) == "char")
for (i in dim(df_train)[2]){
which(class(df_train[,i]) == "char")
}
summary(df_train)
df_train = train
df_train = df_train[ , !names(df_train) %in% c("X13","X88")]
summary(df_train)
lapply(df_train, class)
[lapply(df_train, class)]
# stopCluster(cl)
list(lapply(df_train, class))
array(lapply(df_train, class))
str(lapply(df_train, class))
as.factor(df_train$X8)
?as.factor
as.factor(df_train$X8, levels = c(1,0))
as.factor(df_train$X8)
df_train$X8  = as.factor(df_train$X8)
df_train$X9  = as.factor(df_train$X9)
df_train$X24 = as.factor(df_train$X24)
levels(df_train$X8)
levels(df_train$X8) = c(0,1)
levels(df_train$X9)
levels(df_train$X9) = c(0,1)
levels(df_train$X24)
levels(df_train$X24) = c(0,1)
# dim(df_train)
df_train
library(xgboost)
library(caret)
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
start <- Sys.time()
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
# classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
# max_depth = c(4, 6, 9, 12),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1))
start <- Sys.time()
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
1+1
# dim(df_train)
df_train
# dim(df_train)
str(df_train)
trControl = trainControl(
method = 'cv',
number = 10,
# verboseIter = TRUE,
allowParallel = TRUE)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1))
start <- Sys.time()
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
library(AppliedPredictiveModeling)
pp_hpc <- preProcess(schedulingData[, -8],
method = c("center", "scale", "YeoJohnson"))
pp_hpc <- preProcess(df_train,
method = c("center", "scale", "YeoJohnson"))
transformed <- predict(pp_hpc, newdata = df_train)
View(transformed)
library(xgboost)
library(caret)
1+1
trControl = trainControl(
method = 'cv',
number = 10,
# verboseIter = TRUE,
allowParallel = TRUE)
trControl = trainControl(
method = 'cv',
number = 10)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
eta = c(0.05, 0.1))
# lambda = c(.01, .1),
# alpha = c(.01, .1))
target[,1]
# lambda = c(.01, .1),
# alpha = c(.01, .1))
# target[,1]
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1))
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
is.na(target[,1])
which(is.na(target[,1]) == T)
# xgbmod_2
train(    x = transformed,
y = target[,1],
method = "randomforrests",
metric = "RMSE")
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
gamma = .1,
min_child_weight = c(25),
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
train <- read.csv("./train_data.csv")
target <- read.csv("./target.csv")
head(train)
# train['X18']
train['X88'] = lapply(train['X18'], FUN = substr, start = 1, stop = 5)
# train = train[,! names(train) %in% 'X18']
train = train[,! names(train) %in% c('X18', 'X21', 'X22', 'X23')]
df_train = train
df_train = df_train[ , !names(df_train) %in% c("X13","X88")]
str(lapply(df_train, class)) # 8 9 24
df_train$X8  = as.factor(df_train$X8)
df_train$X9  = as.factor(df_train$X9)
df_train$X24 = as.factor(df_train$X24)
levels(df_train$X8)  = c(0,1)
levels(df_train$X9)  = c(0,1)
levels(df_train$X24) = c(0,1)
# dim(df_train)
str(df_train)
library(AppliedPredictiveModeling)
pp_hpc <- preProcess(df_train,
method = c("center", "scale", "YeoJohnson"))
transformed <- predict(pp_hpc, newdata = df_train)
View(transformed)
trControl = trainControl(
method = 'cv',
number = 10)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
gamma = .1,
min_child_weight = c(25),
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
# which(is.na(target[,1]) == T)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
gamma = .1,
min_child_weight = c(25),
colsample_bytree = c(1),
subsample = c(1)
)
# which(is.na(target[,1]) == T)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
View(df_train)
View(pp_hpc)
View(transformed)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
transformed
write.csv(transformed, "./fin_train.csv", index = F)
write.csv(transformed, "./fin_train.csv", row.names =  = F)
write.csv(transformed, "./fin_train.csv", row.names = F)
trControl = trainControl(
method = 'cv',
number = 10)
library(xgboost)
library(caret)
1+1
library(doParallel)
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
# stopCluster(cl)
trControl = trainControl(
method = 'cv',
number = 10)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
gamma = .1,
min_child_weight = c(25),
colsample_bytree = c(1),
subsample = c(1)
)
# which(is.na(target[,1]) == T)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
train(x = transformed,
y = target[,1],
method = "rf",
metric = "RMSE")
1+1
lr_md = train(x = transformed,
y = target[,1],
method = "lr",
metric = "RMSE",
verbose = T)
lr_md = train(x = transformed,
y = target[,1],
method = "penalized",
metric = "RMSE",
lambda1 = .1,
lambda2 = .1,
verbose = T)
write.csv(transformed, "train_transformed.csv", row.names = F)
lr_md = train(x = transformed,
y = target[,1],
method = "penalized",
metric = "RMSE",
lambda1 = .1,
lambda2 = .1,
verbose = T)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
train <- read.csv("./train_data.csv")
target <- read.csv("./target.csv")
names(df_train)
lapply(cat_col, paste0, "X")
# dim(df_train)
cat_col = c(3, 5, 6, 7, 8, 9, 10, 15, 17)
lapply(cat_col, paste0, "X")
lapply("X", paste0, cat_col)
lapply("X", paste0, cat_col)[[1]]
cat_col = lapply("X", paste0, cat_col)[[1]]
train <- read.csv("./train_data.csv")
target <- read.csv("./target.csv")
train = train[,! names(train) %in% c('X18', 'X21', 'X22', 'X23')]
df_train = train
df_train = df_train[ , !names(df_train) %in% c("X13")]
# str(lapply(df_train, class)) # 8 9 24
df_train$X8  = as.factor(df_train$X8)
df_train$X9  = as.factor(df_train$X9)
df_train$X24 = as.factor(df_train$X24)
levels(df_train$X8)  = c(0,1)
levels(df_train$X9)  = c(0,1)
levels(df_train$X24) = c(0,1)
dim(df_train)
View(df_train)
# dim(df_train)
cat_col = c(3, 5, 6, 7, 8, 9, 10, 15, 17)
cat_col = lapply("X", paste0, cat_col)[[1]]
cat_col
strsplit(cat_col, " ")
separate(cat_col)
(cat_col)
strsplit(cat_col)
strsplit(cat_col, " ")
cat_col = strsplit(cat_col, " ")
names(df_train)
# names(df_train)
fastDummies::dummy_columns(df_train, select_columns = cat_col)
list(strsplit(cat_col, " "))
cat_col
cat_col %>% as.list()
cat_col %>% as.array()
# dim(df_train)
cat_col = c(3, 5, 6, 7, 8, 9, 10, 15, 17)
cat_col = lapply("X", paste0, cat_col)[[1]]
cat_col %>% as.array()
# names(df_train)
fastDummies::dummy_columns(df_train, select_columns = cat_col)
# names(df_train)
df_train = fastDummies::dummy_columns(df_train, select_columns = cat_col)
str(df_train)
target
target[,1]
# depracate
# library(AppliedPredictiveModeling)
# pp_hpc <- preProcess(df_train,
#                      method = c("center", "scale", "YeoJohnson"))
# transformed <- predict(pp_hpc, newdata = df_train)
write.csv(df_train, "./fin_train.csv", row.names = F)
# testdata
# # (['容纳人数', '洗手间数量', '床的数量', '床的类型', '卧室数量', '取消条款', '所在城市', '清洁费',
#        '房主是否有个人资料图片', '房主身份是否验证', '是否支持随即预订', '维度', '经度', '民宿周边', '评论个数',
#        '房产类型', '民宿评分', '房型', '邮编', '价格', '首次评论日期_new', '何时成为房主_new',
#        '最近评论日期_new', '房主回复率_new'],
test = read.csv("./test_data.csv")
test = test[,! names(test) %in% c('X18', 'X21', 'X22', 'X23')]
df_test = test
df_test = df_test[ , !names(df_test) %in% c("X13")]
str(lapply(df_test, class)) # 8 9 24
# str(lapply(df_test, class)) # 8 9 24
df_test$X8  = as.factor(df_test$X8)
df_test$X9  = as.factor(df_test$X9)
df_test$X24 = as.factor(df_test$X24)
levels(df_test$X8)  = c(0,1)
levels(df_test$X9)  = c(0,1)
levels(df_test$X24) = c(0,1)
names(df_test)
cat_col = c(3, 5, 6, 7, 8, 9, 10, 15, 17)
cat_col = lapply("X", paste0, cat_col)[[1]]
cat_col = strsplit(cat_col, " ")
cat_col %>% as.array()
# names(df_train)
df_test = fastDummies::dummy_columns(df_test, select_columns = cat_col)
cat_col = c(3, 5, 6, 7, 8, 9, 10, 15, 17)
cat_col = lapply("X", paste0, cat_col)[[1]]
# cat_col = strsplit(cat_col, " ")
cat_col %>% as.array()
# names(df_train)
df_test = fastDummies::dummy_columns(df_test, select_columns = cat_col)
str(df_test)
# library(AppliedPredictiveModeling)
# pp_hpc2 <- preProcess(df_test,
#                      method = c("center", "scale", "YeoJohnson"))
# transformed2 <- predict(pp_hpc2, newdata = df_test)
write.csv(df_test, "./fin_test.csv", row.names = F)
out1 = read.csv("./out_lgbm.csv")
out1
sample_sub = read.csv("./data/sub.csv")
sample_sub
sample_sub$价格 = out1$X0
sample_sub
write.csv(sample_sub, "./data/sub1_lgbm.csv", row.names = F)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
df_train
library(tidyverse)
df_train %>% filter(df_train$X1 == 1)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
.libPaths()
file.edit('~/.Renviron')
.libPaths()
file.edit('~/.Renviron')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
file.edit('~/.Renviron')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
# library(tidyverse)
file.edit('~/.Renviron')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
ggplot(diamonds) + aes(x = "weight", y = "color")
ggplot(diamonds) + aes(x = "weight", y = "color") + geom_point()
ggplot(diamonds) + aes(x = "weight", y = "colour") + geom_point()
diamonds
ggplot(diamonds) + aes(x = "colour", y = "price") + geom_point()
diamonds
ggplot(diamonds) + aes(x = "color", y = "price") + geom_point()
ggplot(diamonds) + aes(x =color, y = price) + geom_point()
# ggplot(diamonds) + aes(x =color, y = price) + geom_point()
?pie
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
#
?geom_column
?geom_col
ggplot(diamonds)
(diamonds)
diamonds$carat
data.frame(diamonds$carat)
dfff = data.frame(diamonds$carat)
dfff
?geom_smooth()
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(xgboost)
library("Matrix")
train_data = read.csv("./encoded_train.csv")
tests_data = read.csv("./encoded_test.csv")
train_y    = read.csv("./trian_y.csv")
sub = read.csv("./sample_submission.csv")
