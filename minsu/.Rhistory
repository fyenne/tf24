knitr::opts_chunk$set(echo = TRUE)
target <- read.csv("./target.csv")
train <- read.csv("./train_data.csv")
head(train)
train['X15']
head(train)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
train['X18']
gsub("\.\d", "", train['X18'])
gsub("[::punct::]\d", "", train['X18'])
gsub("[[:punct:]]\d", "", train['X18'])
gsub(".\d{2}$", "", train['X18'])
gsub("./d{2}$", "", train['X18'])
head(train)
gsub(".\\d{2}$", "", train['X18'])
gsub(".\\d$", "", train['X18'])
gsub(".\\d$", "", train['X18'])[1]
substr(train['X18'], 1,5)
train['X18']
apply(train['X18'], substr(train['X18'], 1,5))
apply(train['X18'], substr())
apply(train['X18'], substr, c(train['X18'],1,5))
?substr
apply(train['X18'], substr, x = train['X18'], start = 1, stop = 5)
apply(train['X18'], FUN = substr, x = train['X18'], start = 1, stop = 5)
lapply(train['X18'], FUN = substr, x = train['X18'], start = 1, stop = 5)
lapply(train['X18'], FUN = substr,  start = 1, stop = 5)
train['X88'] = lapply(train['X18'], FUN = substr, start = 1, stop = 5)
train['X88']
head(train)
train[-'X18']
train[!'X18']
train[,! names(train) in 'X18']
train[,! names(train) %in% 'X18']
train = train[,! names(train) %in% 'X18']
train
fastDummies::dummy_cols(train)
1+1
restart()
head(train)
train['X18']
head(train)
train[,! names(train) %in% c('X21', 'X22', 'X23')]
train = train[,! names(train) %in% c('X21', 'X22', 'X23')]
head(train)
df_train = fastDummies::dummy_columns(train, select_columns = c("X13", "X24", "X88", "X8", "X9"))
### Prepare xgboost
# features = setdiff(names(data_matrix), c("Season", "DayNum", "T1", "T2", "T1_Points", "T2_Points", "ResultDiff"))
y
### Prepare xgboost
# features = setdiff(names(data_matrix), c("Season", "DayNum", "T1", "T2", "T1_Points", "T2_Points", "ResultDiff"))
target
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
library(xgboost)
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
?trainControl
library(caret)
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4, 6, 9, 12),
eta = c(0.05, 0.1),
# gamma = c(2),
colsample_bytree = c(.8, .7),
subsample = c(0.750),
min_child_weight = c(25))
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
# max_depth = c(4, 6, 9, 12),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1),
colsample_bytree = c(.8, .7),
subsample = c(0.750),
min_child_weight = c(25))
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
# max_depth = c(4, 6, 9, 12),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1),
colsample_bytree = c(.8, .7),
subsample = c(0.750))
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
xgbmod_2 <- train(
x = df_train,
y = target,
method = 'xgbLinear',
metric = "rmse",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
### Prepare xgboost
# features = setdiff(names(data_matrix), c("Season", "DayNum", "T1", "T2", "T1_Points", "T2_Points", "ResultDiff"))
target[,1]
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
library(doParallel)
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
# stopCluster(cl)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
# max_depth = c(4, 6, 9, 12),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1),
colsample_bytree = c(.8, .7),
subsample = c(0.750))
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "rmse",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
# max_depth = c(4, 6, 9, 12),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1))
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "rmse",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
xgbmod_2
### Prepare xgboost
# features = setdiff(names(data_matrix), c("Season", "DayNum", "T1", "T2", "T1_Points", "T2_Points", "ResultDiff"))
# target[,1]
df_train
### Prepare xgboost
# features = setdiff(names(data_matrix), c("Season", "DayNum", "T1", "T2", "T1_Points", "T2_Points", "ResultDiff"))
# target[,1]
is.na(df_train)
train <- read.csv("./train_data.csv")
target <- read.csv("./target.csv")
is.na(train)
is.na(train) == T
which(is.na(train) == T)
which(len(train) == 0)
which(length(train) == 0)
which(length(train) == 2)
which(length(train) == 11)
length(train)
length(train['X18'])
lapply(train['X18'], length
lapply(train['X18'], length)
head(train)
train['X18']
train['X88'] = lapply(train['X18'], FUN = substr, start = 1, stop = 5)
# train = train[,! names(train) %in% 'X18']
train = train[,! names(train) %in% c('X18', 'X21', 'X22', 'X23')]
View(train)
df_train = df_train[ , !names(df_train) %in% c("X13","X88")]
type(df_train)
class(df_train)
class(df_train$X4)
for (i in length(df_train)){
class(df_train[,i]) == "char"
}
which(class(df_train[,i]) == "char")
for (i in length(df_train)){
which(class(df_train[,i]) == "char")
}
which(class(df_train[,i]) == "cha")
for (i in length(df_train)){
which(class(df_train[,i]) == "cha")
}
length(df_train)
dim(df_train)
train <- read.csv("./train_data.csv")
target <- read.csv("./target.csv")
head(train)
# train['X18']
train['X88'] = lapply(train['X18'], FUN = substr, start = 1, stop = 5)
# train = train[,! names(train) %in% 'X18']
train = train[,! names(train) %in% c('X18', 'X21', 'X22', 'X23')]
df_train = df_train[ , !names(df_train) %in% c("X13","X88")]
for (i in length(df_train)){
which(class(df_train[,i]) == "cha")
}
for (i in dim(df_train)){
which(class(df_train[,i]) == "cha")
}
for (i in dim(df_train)[2]){
which(class(df_train[,i]) == "cha")
}
which(class(df_train[,i]) == "char")
for (i in dim(df_train)[2]){
which(class(df_train[,i]) == "char")
}
summary(df_train)
df_train = train
df_train = df_train[ , !names(df_train) %in% c("X13","X88")]
summary(df_train)
lapply(df_train, class)
[lapply(df_train, class)]
# stopCluster(cl)
list(lapply(df_train, class))
array(lapply(df_train, class))
str(lapply(df_train, class))
as.factor(df_train$X8)
?as.factor
as.factor(df_train$X8, levels = c(1,0))
as.factor(df_train$X8)
df_train$X8  = as.factor(df_train$X8)
df_train$X9  = as.factor(df_train$X9)
df_train$X24 = as.factor(df_train$X24)
levels(df_train$X8)
levels(df_train$X8) = c(0,1)
levels(df_train$X9)
levels(df_train$X9) = c(0,1)
levels(df_train$X24)
levels(df_train$X24) = c(0,1)
# dim(df_train)
df_train
library(xgboost)
library(caret)
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
start <- Sys.time()
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
trControl = trainControl(
method = 'cv',
number = 10,
# summaryFunction = giniSummary,
# classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
# max_depth = c(4, 6, 9, 12),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1))
start <- Sys.time()
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
1+1
# dim(df_train)
df_train
# dim(df_train)
str(df_train)
trControl = trainControl(
method = 'cv',
number = 10,
# verboseIter = TRUE,
allowParallel = TRUE)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1))
start <- Sys.time()
xgbmod_2 <- train(
x = df_train,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
# metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
library(AppliedPredictiveModeling)
pp_hpc <- preProcess(schedulingData[, -8],
method = c("center", "scale", "YeoJohnson"))
pp_hpc <- preProcess(df_train,
method = c("center", "scale", "YeoJohnson"))
transformed <- predict(pp_hpc, newdata = df_train)
View(transformed)
library(xgboost)
library(caret)
1+1
trControl = trainControl(
method = 'cv',
number = 10,
# verboseIter = TRUE,
allowParallel = TRUE)
trControl = trainControl(
method = 'cv',
number = 10)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
eta = c(0.05, 0.1))
# lambda = c(.01, .1),
# alpha = c(.01, .1))
target[,1]
# lambda = c(.01, .1),
# alpha = c(.01, .1))
# target[,1]
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
eta = c(0.05, 0.1),
lambda = c(.01, .1),
alpha = c(.01, .1))
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbLinear',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
is.na(target[,1])
which(is.na(target[,1]) == T)
# xgbmod_2
train(    x = transformed,
y = target[,1],
method = "randomforrests",
metric = "RMSE")
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
gamma = .1,
min_child_weight = c(25),
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
train <- read.csv("./train_data.csv")
target <- read.csv("./target.csv")
head(train)
# train['X18']
train['X88'] = lapply(train['X18'], FUN = substr, start = 1, stop = 5)
# train = train[,! names(train) %in% 'X18']
train = train[,! names(train) %in% c('X18', 'X21', 'X22', 'X23')]
df_train = train
df_train = df_train[ , !names(df_train) %in% c("X13","X88")]
str(lapply(df_train, class)) # 8 9 24
df_train$X8  = as.factor(df_train$X8)
df_train$X9  = as.factor(df_train$X9)
df_train$X24 = as.factor(df_train$X24)
levels(df_train$X8)  = c(0,1)
levels(df_train$X9)  = c(0,1)
levels(df_train$X24) = c(0,1)
# dim(df_train)
str(df_train)
library(AppliedPredictiveModeling)
pp_hpc <- preProcess(df_train,
method = c("center", "scale", "YeoJohnson"))
transformed <- predict(pp_hpc, newdata = df_train)
View(transformed)
trControl = trainControl(
method = 'cv',
number = 10)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
gamma = .1,
min_child_weight = c(25),
colsample_bytree = c(.8, .9),
subsample = c(0.750)
)
# which(is.na(target[,1]) == T)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
tuneGridXGB = expand.grid(
nrounds = c(300, 500),
max_depth = c(4,6,9,13),
eta = c(0.05, 0.1),
# lambda = c(.01, .1),
# alpha = c(.01, .1)
gamma = .1,
min_child_weight = c(25),
colsample_bytree = c(1),
subsample = c(1)
)
# which(is.na(target[,1]) == T)
start <- Sys.time()
xgbmod_2 <- train(
x = transformed,
y = target[,1],
method = 'xgbTree',
metric = "RMSE",
trControl = trControl,
tuneGrid = tuneGridXGB,
verbose = T)
